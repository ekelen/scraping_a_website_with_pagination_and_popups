{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb36370-83e4-43c4-bdfe-f5f16110e1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import csv\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logging.debug(\"test\")\n",
    "\n",
    "# Maximum number of pages to scrape\n",
    "MAX_PAGES = 0  # Set to 0 for no limit, or specify a number\n",
    "\n",
    "# Rate limits: 2 requests per second\n",
    "RATE_LIMIT = 2\n",
    "RATE_PERIOD = 1\n",
    "\n",
    "# User-Agent header\n",
    "USER_AGENT = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "\n",
    "# Headers with the User-Agent\n",
    "headers = {\"User-Agent\": USER_AGENT}\n",
    "\n",
    "# Params for in-person therapy\n",
    "\n",
    "\n",
    "# Rate limit decorator\n",
    "@sleep_and_retry\n",
    "@limits(calls=RATE_LIMIT, period=RATE_PERIOD)\n",
    "\n",
    "# Function to fetch url\n",
    "def fetch_url(url):\n",
    "    return requests.get(url, headers=headers)\n",
    "\n",
    "# Function to extract data about all therapists in a specific province\n",
    "def scrape_city_data(province, city):\n",
    "    city_url = f\"https://www.psychologytoday.com/ca/therapists/{province}/{city}?category=in-person\"\n",
    "    response = fetch_url(city_url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Get the total number of pages for the province\n",
    "        num_pages = get_num_pages(city_url)\n",
    "        logger.debug('city url: ', city_url, 'num_pages:', num_pages)\n",
    "\n",
    "        therapists_data = []\n",
    "\n",
    "        # Iterate over each page and scrape therapist data\n",
    "        for page_num in range(1, num_pages + 1):\n",
    "            page_url = f\"{city_url}&page={page_num}\"\n",
    "            response = fetch_url(page_url)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                therapists_data.extend(scrape_page_data(soup, province))\n",
    "                save_to_csv(therapists_data, city)\n",
    "                print(f\"Page {page_num} scraped and added to file {city}.csv\")\n",
    "            else:\n",
    "                print(f\"Failed to fetch page: {page_url}\")\n",
    "\n",
    "        return therapists_data\n",
    "    else:\n",
    "        print(f\"Failed to fetch city data: {city_url}\")\n",
    "        return []\n",
    "\n",
    "# Function to find the number of pages of a specific category\n",
    "def get_num_pages(url):\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        # Find the div tag for pagination\n",
    "        pagination_div = soup.find('div', class_='results-pagination-container')\n",
    "        if pagination_div:\n",
    "            pages = pagination_div.find_all('a')\n",
    "            highest_page_number = 0\n",
    "            new_highest_page_number = True\n",
    "\n",
    "            # Find the highest page number in the current page and replace it with the new highest page number\n",
    "            # until there are no more therapist pages\n",
    "            while new_highest_page_number and (highest_page_number < MAX_PAGES or MAX_PAGES == 0):\n",
    "                new_highest_page_number = False\n",
    "                for page in pages:\n",
    "                    try:\n",
    "                        # Find the page numbers on the current page\n",
    "                        page_number = int(page.text)\n",
    "                        # If there is a new page number higher than the current highest page number,\n",
    "                        # Find the highest page number on the current page\n",
    "                        if page_number > highest_page_number:\n",
    "                            highest_page_number = page_number\n",
    "                            page_url = page['href']\n",
    "                            response = requests.get(page_url, headers=headers)\n",
    "                            response.raise_for_status()\n",
    "                            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                            pagination_div = soup.find('div', class_='results-pagination-container')\n",
    "                            if pagination_div:\n",
    "                                pages = pagination_div.find_all('a')\n",
    "                                new_highest_page_number = True\n",
    "                                break\n",
    "                    except (ValueError, KeyError) as e:\n",
    "                        continue\n",
    "\n",
    "            num_pages = highest_page_number\n",
    "        # If there are no pagination tags, then it means there is only one or a few therapists on the landing page\n",
    "        else:\n",
    "            num_pages = 1\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting number of pages: {e}\")\n",
    "        num_pages = 1\n",
    "\n",
    "    return num_pages\n",
    "\n",
    "# Function to extract data about the therapists on a specific page\n",
    "def scrape_page_data(soup, province):\n",
    "    therapists_data = []\n",
    "    results_container = soup.find('div', class_='results')\n",
    "    if results_container:\n",
    "        # Find all therapists on the results page\n",
    "        therapists = results_container.find_all('div', class_='results-row')\n",
    "        logger.debug(\"Found \" + str(len(therapists)) + \" therapists on the page.\")\n",
    "        if therapists:\n",
    "            for therapist in therapists:\n",
    "                # Find the tag for the view button and scrape the therapist's url\n",
    "                view_button = therapist.find('a', class_='profile-title')\n",
    "                if view_button:\n",
    "                    view_url = view_button['href']\n",
    "                    response = fetch_url(view_url)\n",
    "                    if response.status_code == 200:\n",
    "                        # Scrape the therapist's data\n",
    "                        therapist_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                        therapist_data = scrape_therapist_data(therapist_soup, province)\n",
    "                        therapists_data.append(therapist_data)\n",
    "                    else:\n",
    "                        print(f\"Failed to fetch therapist page: {view_url}\")\n",
    "                else:\n",
    "                    print(\"View button not found.\")\n",
    "        else:\n",
    "            print(\"No therapist elements found.\")\n",
    "    else:\n",
    "        print(\"Results container not found.\")\n",
    "    return therapists_data\n",
    "\n",
    "# Function to extract a specific therapist's data\n",
    "def scrape_therapist_data(soup, province):\n",
    "    therapist_data = {}\n",
    "    therapist_data['province'] = province  # Add the province to the data\n",
    "    # TODO: Get rid of\n",
    "    meta_section = soup.find('div', class_='breadcrumb-xs-hide')\n",
    "    if meta_section:\n",
    "        meta_section = meta_section.contents\n",
    "    \n",
    "    # Extract the city\n",
    "    city_div = soup.find('a', {'data-x': 'breadcrumb-City'}).find('div', {'itemprop': 'name'})\n",
    "    if city_div:\n",
    "        therapist_data['city'] = city_div.text.strip()\n",
    "\n",
    "    address_divs = soup.find_all('div', class_='address')\n",
    "\n",
    "    # Extract the street addresses\n",
    "    street_addresses = []\n",
    "    for address_div in address_divs:\n",
    "        street_address = address_div.find('p', class_='address-line')\n",
    "        if street_address:\n",
    "            street_addresses.append(street_address.text.strip())\n",
    "\n",
    "    # Assign street_address_1 and street_address_2 accordingly\n",
    "    if len(street_addresses) >= 1:\n",
    "        therapist_data['street_address_1'] = street_addresses[0]\n",
    "    if (len(street_addresses) >= 2) & (street_addresses[0] != street_addresses[1]):\n",
    "        therapist_data['street_address_2'] = street_addresses[1]\n",
    "    else:\n",
    "        therapist_data['street_address_2'] = None\n",
    "\n",
    "    # Extract the zip codes\n",
    "    zip_codes = []\n",
    "    for address_div in address_divs:\n",
    "        zip_span = address_div.find('span')\n",
    "        if zip_span:\n",
    "            zip_code = zip_span.text.split()[-1]\n",
    "            zip_codes.append(zip_code)\n",
    "\n",
    "    # Assign zip_1 and zip_2 accordingly\n",
    "    if len(zip_codes) >= 1:\n",
    "        therapist_data['zip_1'] = zip_codes[0]\n",
    "    if (len(zip_codes) >= 2) & (zip_codes[0] != zip_codes[1]):\n",
    "        therapist_data['zip_2'] = zip_codes[1]\n",
    "    else:\n",
    "        therapist_data['zip_2'] = None\n",
    "\n",
    "    # Extract the business or person name\n",
    "    business_name = soup.find('h1', attrs={'class': 'profile-title'})\n",
    "    if business_name:\n",
    "        text_business_name = list(business_name.stripped_strings)\n",
    "        therapist_data['person/business_name'] = ' '.join(text_business_name)\n",
    "    else:\n",
    "        raise ValueError(\"Business or person name not found in the profile.\")\n",
    "\n",
    "    # Extract the title\n",
    "    title = soup.find('h2', class_='profile-suffix-heading')\n",
    "    if title:\n",
    "        suffix_containers = title.find_all('span', class_='profile-suffix-container')\n",
    "        title_text = ''\n",
    "        for container in suffix_containers:\n",
    "            suffixes = container.find_all('span', class_='glossary-tooltip-link')\n",
    "            for suffix in suffixes:\n",
    "                title_text += suffix.text.strip() + ', '\n",
    "        therapist_data['title'] = title_text[:-2]  # Remove the trailing comma and space\n",
    "\n",
    "    # Extract the telephone\n",
    "    telephone_span = soup.find('a', attrs={'class': 'phone-icon-ctr'})\n",
    "    if telephone_span:\n",
    "        therapist_data['telephone'] = telephone_span.text.strip()\n",
    "    else:\n",
    "        therapist_data['telephone'] = None\n",
    "\n",
    "    # Extract the insurance providers\n",
    "    insurance = soup.find('div', class_='insurance')\n",
    "    if insurance:\n",
    "        insurance_list = []\n",
    "        insurance_spans = insurance.find_all('span')\n",
    "        for span in insurance_spans:\n",
    "            insurance_list.append(span.text.strip())\n",
    "        therapist_data['insurance'] = insurance_list\n",
    "    else:\n",
    "        therapist_data['insurance'] = None\n",
    "\n",
    "    # Extract the specialties and expertise\n",
    "    specialties_and_expertise = []\n",
    "    specialty_attributes_section = soup.find('div', class_='specialty-attributes-section')\n",
    "    if specialty_attributes_section:\n",
    "        attributes_groups = specialty_attributes_section.find_all('div', class_='attributes-group')\n",
    "        for group in attributes_groups:\n",
    "            attributes_list = group.find_all('span', class_='attribute_base')\n",
    "            for attribute in attributes_list:\n",
    "                specialties_and_expertise.append(attribute.text.strip())\n",
    "    therapist_data['specialties_and_expertise'] = specialties_and_expertise\n",
    "\n",
    "    # Extract price\n",
    "\n",
    "    other_price_div = soup.find('div', class_='fees')\n",
    "    \n",
    "    if other_price_div:\n",
    "        price_text = other_price_div.get_text()\n",
    "        text_list = price_text.replace('\\n\\n','').splitlines()\n",
    "        text_list = str([text.strip() for text in text_list if text.strip()])  # Remove empty strings\n",
    "\n",
    "        individual = re.search(r'Individual Sessions \\$([0-9]+)', text_list)\n",
    "        couple = re.search(r'Couple Sessions \\$([0-9]+)', text_list)\n",
    "\n",
    "        individual_price = individual.group(1) if individual else None\n",
    "        couple_price = couple.group(1) if couple else None\n",
    "\n",
    "        print(\"Individual:\", individual_price)\n",
    "        print(\"Couple:\", couple_price)\n",
    "\n",
    "\n",
    "        therapist_data['individual_price'] = individual_price\n",
    "        therapist_data['couple_price'] = couple_price\n",
    "    else:\n",
    "        therapist_data['individual_price'] = None\n",
    "        therapist_data['couple_price'] = None\n",
    "\n",
    "            \n",
    "\n",
    "    # Extract types of therapy\n",
    "    types_of_therapy = []\n",
    "    therapy_group_divs = soup.find_all('div', class_='attributes-group')\n",
    "    for div in therapy_group_divs:\n",
    "        if div.find('h3', class_='attributes-group-title').text.strip() == 'Types of Therapy':\n",
    "            ul_tag = div.find('ul', class_='section-list')\n",
    "            if ul_tag:\n",
    "                li_tags = ul_tag.find_all('li')\n",
    "                for li_tag in li_tags:\n",
    "                    span_tag = li_tag.find('span', class_='attribute_base')\n",
    "                    if span_tag:\n",
    "                        types_of_therapy.append(span_tag.text.strip())\n",
    "\n",
    "    therapist_data['types_of_therapy'] = types_of_therapy\n",
    "\n",
    "    # Extract age\n",
    "    ages = []\n",
    "    client_focus_div = soup.find('div', class_='client-focus-container-small')\n",
    "    if client_focus_div:\n",
    "        age_divs = client_focus_div.find_all('div', class_='client-focus-tile')\n",
    "        for age_div in age_divs:\n",
    "            if 'Age' in age_div.text:\n",
    "                age_items = age_div.find_all('div', class_='client-focus-item')\n",
    "                for item in age_items:\n",
    "                    age_span = item.find('span', class_='client-focus-description')\n",
    "                    if age_span:\n",
    "                        ages.append(age_span.text.strip().rstrip(' ,'))  # Remove trailing comma\n",
    "\n",
    "    therapist_data['ages'] = ages\n",
    "\n",
    "    return therapist_data\n",
    "    \n",
    "# Function to save data to csv files, per province\n",
    "def save_to_csv(data, city):\n",
    "    filename = f'therapists_{city}.csv'\n",
    "    openmethod = 'a' if os.path.exists(filename) else 'w'\n",
    "    if data:\n",
    "        with open(filename, openmethod, newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=data[0].keys())\n",
    "            if openmethod == 'w':\n",
    "                writer.writeheader()\n",
    "            for therapist in data:\n",
    "                writer.writerow(therapist)\n",
    "    else:\n",
    "        with open(filename, openmethod, newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            if openmethod == 'w':\n",
    "                writer.writerow(['province', 'city', 'street_address_1', 'street_address_2', 'zip_1', 'zip_2', 'person/business_name', 'title', 'telephone', 'insurance', 'specialties_and_expertise', 'individual_price', 'couple_price', 'types_of_therapy', 'ages'])\n",
    "                \n",
    "# Function to scrape data from Psychologytoday.com\n",
    "def main():\n",
    "    provinces = [\"on\"]\n",
    "    cities = [\"ottawa\"]\n",
    "\n",
    "    for province in provinces:\n",
    "        for city in cities:\n",
    "            scrape_city_data(province, city)\n",
    "    print(\"Scraping completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
